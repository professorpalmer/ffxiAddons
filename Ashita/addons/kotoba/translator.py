#!/usr/bin/env python3
"""
Kotoba External Translator - DeepL Edition
Watches translation_queue.txt and translates text using DeepL API
Enhanced with MMO/FFXI context awareness via custom glossary + community terms

Features:
- DeepL API integration for high-quality translations
- Custom FFXI glossary (500+ terms from FFXIclopedia)
- Community glossary file support (ffxi_glossary.txt)
- Auto-reload glossary without restart
- Untranslated term detection
- Translation stats tracking
- Casual tone post-processing

FFXI terminology references:
https://ffxiclopedia.fandom.com/wiki/Final_Fantasy_XI_Dictionary_of_Terms_and_Slang
"""

import os
import time
import sys
import re
from pathlib import Path
from datetime import datetime

try:
    import deepl
    print("[Kotoba Translator] DeepL library loaded successfully")
except ImportError:
    print("[Kotoba Translator] ERROR: deepl not installed!")
    print("[Kotoba Translator] Install with: pip install deepl")
    sys.exit(1)

# Load DeepL API key from config
CONFIG_FILE = Path(__file__).parent / "translator_config.txt"
DEEPL_API_KEY = None

try:
    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            if line.startswith('DEEPL_API_KEY='):
                DEEPL_API_KEY = line.split('=', 1)[1].strip()
                break
    
    if not DEEPL_API_KEY:
        raise ValueError("DEEPL_API_KEY not found in config")
    
    translator = deepl.Translator(DEEPL_API_KEY)
    print(f"[Kotoba Translator] DeepL initialized successfully")
    
except Exception as e:
    print(f"[Kotoba Translator] ERROR: Could not load DeepL API key: {e}")
    print(f"[Kotoba Translator] Make sure translator_config.txt contains: DEEPL_API_KEY=your_key_here")
    sys.exit(1)

# File paths
SCRIPT_DIR = Path(__file__).parent
QUEUE_FILE = SCRIPT_DIR / "translation_queue.txt"
RESULTS_FILE = SCRIPT_DIR / "translation_results.txt"
COMMUNITY_GLOSSARY_FILE = SCRIPT_DIR / "ffxi_glossary.txt"
SUGGESTED_TERMS_FILE = SCRIPT_DIR / "suggested_terms.log"

# Translation cache
cache = {}

# Stats tracking
stats = {
    'translations': 0,
    'cache_hits': 0,
    'cache_misses': 0,
    'glossary_terms_used': 0,
    'start_time': time.time()
}

# Glossary metadata
glossary_last_modified = 0

# FFXI/MMO Glossary - Map JP terms to natural EN equivalents
FFXI_GLOSSARY = {
    # Endgame Activities
    'ã‚½ãƒ¼ãƒ†ã‚£': 'Sortie',
    'ã‚ªãƒ‡ã‚·ãƒ¼': 'Odyssey',
    'ã‚ªãƒ¼ãƒ¡ãƒ³': 'Omen',
    'ã‚¢ãƒ³ãƒã‚¹': 'Ambu',
    'ã‚¢ãƒ³ãƒã‚¹ã‚±ãƒ¼ãƒ‰': 'Ambuscade',
    'ãƒ‡ãƒ¥ãƒŠãƒŸã‚¹': 'Dyna',
    'ãƒ€ã‚¤ãƒŠãƒŸã‚¹': 'Dynamis',
    'ãƒ€ã‚¤ãƒãƒ¼': 'Dynamis Divergence',
    'ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹': 'Dynamis Divergence',
    'ã‚¢ãƒ“ã‚»ã‚¢': 'Abyssea',
    'ãƒªãƒ³ãƒã‚¹': 'Limbus',
    'ãƒ†ãƒ¡ãƒŠã‚¹': 'Temenos',
    'ã‚¢ãƒãƒªã‚ªãƒ³': 'Apollyon',
    'ãƒ´ã‚¡ã‚°ãƒª': 'Vagary',
    'ãƒ™ã‚¬ãƒªãƒ¼': 'Vagary',
    
    # Jobs (common JP abbreviations)
    'æˆ¦å£«': 'WAR',
    'ãƒ¢ãƒ³ã‚¯': 'MNK',
    'ç™½é­”': 'WHM',
    'é»’é­”': 'BLM',
    'èµ¤é­”': 'RDM',
    'ã‚·ãƒ¼ãƒ•': 'THF',
    'ãƒŠã‚¤ãƒˆ': 'PLD',
    'æš—é»’': 'DRK',
    'ç£ä½¿ã„': 'BST',
    'åŸéŠè©©äºº': 'BRD',
    'ç‹©äºº': 'RNG',
    'ä¾': 'SAM',
    'å¿è€…': 'NIN',
    'ç«œé¨å£«': 'DRG',
    'å¬å–š': 'SMN',
    'é’é­”': 'BLU',
    'ã‚³ãƒ«ã‚»ã‚¢': 'COR',
    'ã‹ã‚‰ãã‚Š': 'PUP',
    'è¸Šã‚Šå­': 'DNC',
    'å­¦è€…': 'SCH',
    'é¢¨æ°´å£«': 'GEO',
    'é­”å°å‰£å£«': 'RUN',
    
    # Common party/activity phrases (with proper context)
    # These will be handled specially in preprocessing
    
    # Time expressions
    'ä»Šã‹ã‚‰': ' now',
    'ã„ã¾': ' now',
    'ä»Š': ' now',
    
    # Day of week abbreviations (single kanji with trailing space)
    'æœˆ': 'Mon ',
    'ç«': 'Tue ',
    'æ°´': 'Wed ',
    'æœ¨': 'Thu ',
    'é‡‘': 'Fri ',
    'åœŸ': 'Sat ',
    'æ—¥': 'Sun ',
    'å¾Œã§': ' later',
    'ã‚ã¨ã§': ' later',
    
    # Party/Group terms
    'ãƒ‘ãƒ¼ãƒ†ã‚£': 'party',
    'ï¼°ï¼´': 'pt',
    'ã‚¢ãƒ©': 'alliance',
    'ã‚¢ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹': 'alliance',
    'å‹Ÿé›†': 'recruiting',
    'å‚åŠ ': 'join',
    
    # Common game terms
    'ãƒ¬ãƒ™ãƒ«': 'level',
    'ï¼¬ï¼¶': 'lv',
    'ã‚¸ãƒ§ãƒ–': 'job',
    'ã‚¨ãƒªã‚¢': 'area',
    'ã‚¾ãƒ¼ãƒ³': 'zone',
    'ãƒ‰ãƒ­ãƒƒãƒ—': 'drop',
    'ã‚¢ã‚¤ãƒ†ãƒ ': 'item',
    'è£…å‚™': 'gear',
    
    # Status/Requests
    'å‹Ÿé›†ä¸­': 'LFM',  # Looking for more
    'å‚åŠ å¸Œæœ›': 'LFG',  # Looking for group
    'ãŠé¡˜ã„': 'pls',
    'ãŠã­ãŒã„': 'please',
    'ãã ã•ã„': 'pls',
    'æ‰‹ä¼ã£ã¦': 'help',
    'åŠ©ã‘ã¦': 'help',
    
    # Politeness/Chat
    'ãŠï½‹': 'ok',
    'ãŠã‘': 'ok',
    'ã‚Šã‚‡': 'gotcha',
    'ã‚Šã‚‡ã†ã‹ã„': 'roger',
    'ã†ãƒ': 'yep',
    'ãŠã¤': 'thanks',
    'ãŠã¤ã‹ã‚Œ': 'gj',
    'ã‚ã‚ŠãŒã¨': 'ty',
    'ã‚ã‚ŠãŒã¨ã†': 'thanks',
    'ã™ã¾ã‚“': 'sorry',
    'ã”ã‚ã‚“': 'sry',
    'ã”ã‚ã‚“ãªã•ã„': 'sorry',
    'ã‚ˆã‚': 'nice to meet ya',
    'ã‚ˆã‚ã—ã': 'pleased to meet you',
    
    # Additional chat/emotes from FFXIclopedia
    'ã‚ã‚‰ã„': 'lol',
    'ãƒ¯ãƒ­ã‚¿': 'lmao',
    'è‰': 'lmao',
    'ã¯ã„': 'yes',
    'ã„ã„ãˆ': 'no',
    'ã ã‚': 'no good',
    'ã†ã‚“': 'yeah',
    'ã†ã†ã‚“': 'nope',
    'ã™ã': 'right now',
    'ã¡ã‚‡ã£ã¨': 'sec',
    'ã¾ã£ã¦': 'wait',
    'å¾…ã£ã¦': 'wait',
    'ã„ã„ã‚ˆ': 'sure',
    'ãŠã£ã‘ãƒ¼': 'ok',
    'ãŠï½‹': 'ok',
    'ã‚ã‹ã£ãŸ': 'got it',
    'äº†è§£': 'roger',
    'ã„ãã‚ˆ': 'going',
    'æ‰‹ä¼ã£ã¦': 'help',
    'åŠ©ã‘ã¦': 'help',
    'æ•™ãˆã¦': 'teach me',
    'è¦‹ã›ã¦': 'show me',
    'è²¸ã—ã¦': 'lend me',
    'ã¡ã‚‡ã†ã ã„': 'gimme',
    'å¤§ä¸ˆå¤«': 'ok',
    'å•é¡Œãªã„': 'no problem',
    'ã™ã¿ã¾ã›ã‚“': 'excuse me',
    'ã”ã‚ã‚“ã­': 'sorry',
    'ãŠç–²ã‚Œæ§˜': 'gj',
    'é ‘å¼µã£ã¦': 'gl',
    'æ°—ã‚’ã¤ã‘ã¦': 'be careful',
    'ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™': 'thanks',
    'ã©ã†ã„ãŸã—ã¾ã—ã¦': 'np',
    'ã‚ã‹ã‚Šã¾ã—ãŸ': 'understood',
    'ã‚„ã°ã„': 'sick',
    'ã™ã”ã„': 'amazing',
    'ã‹ã£ã“ã„ã„': 'cool',
    'ãˆãã„': 'insane',
    'ãƒã‚¸': 'fr',
    'ã‚¬ãƒ': 'fr',
    'ã†ã': 'no way',
    'æœ¬å½“': 'really',
    'ãªã‚‹ã»ã©': 'I see',
    
    # Battle terms
    'ã‚¿ãƒ³ã‚¯': 'tank',
    'ç›¾': 'tank',
    'å‰è¡›': 'DD',
    'å¾Œè¡›': 'support',
    'ãƒ’ãƒ¼ãƒ©ãƒ¼': 'healer',
    'å›å¾©': 'heal',
    'è£œåŠ©': 'support',
    'å¼·åŒ–': 'buff',
    'å¼±ä½“': 'debuff',
    'é‡£ã‚Š': 'pull',
    'é‡£ã‚Šå½¹': 'puller',
    'ãƒ˜ã‚¤ãƒˆ': 'hate',
    'æ•µè¦–': 'enmity',
    'é€£æº': 'SC',
    'ãƒã‚¸ãƒ': 'MB',
    'ãƒã‚¸ãƒƒã‚¯ãƒãƒ¼ã‚¹ãƒˆ': 'Magic Burst',
    'ã‚¦ã‚§ãƒãƒ³ã‚¹ã‚­ãƒ«': 'WS',
    'ã‚¢ãƒ“ãƒªãƒ†ã‚£': 'ability',
    'ã‚¢ãƒ“': 'ability',
    'ç¯„å›²': 'AoE',
    'å…¨ä½“': 'AoE',
    
    # Party recruitment
    'ãƒ¡ãƒ³ãƒãƒ¼': 'member',
    'é‡è‰¯': 'PUG',
    'å›ºå®š': 'static',
    'ãƒ¬ãƒ™ãƒ«ä¸Šã’': 'leveling',
    'çµŒé¨“å€¤': 'exp',
    'ãƒ¡ãƒªãƒ': 'merit',
    'ãƒªãƒŸãƒƒãƒˆ': 'LB',
    'é™ç•Œ': 'LB',
    
    # Items
    'æ­¦å™¨': 'weapon',
    'é˜²å…·': 'armor',
    'ã‚¢ã‚¯ã‚»': 'accessory',
    'æŒ‡è¼ª': 'ring',
    'å€‰åº«': 'mule',
    'ãƒã‚¶ãƒ¼': 'bazaar',
    'ã‚®ãƒ«': 'gil',
    # 'é‡‘': 'gil',  # Removed: conflicts with Friday (é‡‘æ›œæ—¥)
    'é«˜ã„': 'expensive',
    'å®‰ã„': 'cheap',
    
    # Magic
    'ç™½é­”æ³•': 'WHM magic',
    'é»’é­”æ³•': 'BLM magic',
    'ç²¾éœŠ': 'elemental',
    'ãƒªã‚¸ã‚§ãƒ': 'Regen',
    'ãƒªãƒ•ãƒ¬ã‚·ãƒ¥': 'Refresh',
    'ãƒ˜ã‚¤ã‚¹ãƒˆ': 'Haste',
    'ãƒ—ãƒ­ãƒ†ã‚¹': 'Protect',
    'ã‚·ã‚§ãƒ«': 'Shell',
    'ã‚¹ãƒˆãƒ³ã‚¹ã‚­ãƒ³': 'Stoneskin',
    'ç©ºè‰': 'Utsusemi',
    'ã‚¹ãƒ‹ãƒ¼ã‚¯': 'Sneak',
    'ã‚¤ãƒ³ãƒ“ã‚¸': 'Invis',
    'é€æ˜': 'Invis',
    'ãƒªãƒ¬ã‚¤ã‚º': 'Reraise',
    'ãƒ¬ã‚¤ã‚º': 'Raise',
    'ãƒ†ãƒ¬ãƒ': 'Tele',
    'ãƒ‡ã‚¸ãƒ§ãƒ³': 'Warp',
    'ã‚¹ãƒªãƒ—ãƒ«': 'Sleep',
    'ç¡çœ ': 'Sleep',
    'ã‚¹ã‚¿ãƒ³': 'Stun',
    'ãƒã‚¤ãƒ³ãƒ‰': 'Bind',
    'ã‚°ãƒ©ãƒ“ãƒ‡': 'Gravity',
    'é™å¯‚': 'Silence',
    'æš—é—‡': 'Blind',
    'éº»ç—º': 'Para',
    'ã‚¹ãƒ­ã‚¦': 'Slow',
    
    # Monsters
    'ã‚¹ã‚±ãƒ«ãƒˆãƒ³': 'Skeleton',
    'ã‚¾ãƒ³ãƒ“': 'Zombie',
    'ã‚´ãƒ¼ã‚¹ãƒˆ': 'Ghost',
    'ã‚¯ã‚¥ãƒ€ãƒ•': 'Quadav',
    'ã‚¤ãƒ³ãƒ—': 'Imp',
    'ãƒãƒ³ãƒ‰ãƒ©': 'Mandragora',
    'ã‚µãƒœãƒ†ãƒ³': 'Cactuar',
    'ã‚­ãƒã‚³': 'Funguar',
    'ã‚¹ãƒ©ã‚¤ãƒ ': 'Slime',
    
    # Directions
    'åŒ—': 'north',
    'å—': 'south',
    'æ±': 'east',
    'è¥¿': 'west',
    'è¿‘ã': 'nearby',
    'ä¸­': 'inside',
    'å¤–': 'outside',
    
    # Questions
    'ä½•': 'what',
    'èª°': 'who',
    'ã©ã“': 'where',
    'ã„ã¤': 'when',
    'ãªãœ': 'why',
    'ã©ã†': 'how',
    'ã©ã‚Œ': 'which',
    'ã„ãã¤': 'how many',
    'ã„ãã‚‰': 'how much',
    
    # Actions
    'æ¬²ã—ã„': 'want',
    'å¿…è¦': 'need',
    'ã„ã‚‰ãªã„': 'don\'t need',
    'ã§ãã‚‹': 'can',
    'ã§ããªã„': 'cannot',
    'çŸ¥ã£ã¦ã‚‹': 'know',
    'çŸ¥ã‚‰ãªã„': 'dunno',
    'ã‚ã‹ã‚‹': 'understand',
    'ã‚ã‹ã‚‰ãªã„': 'don\'t understand',
}

# Post-processing patterns to make translations more casual
CASUAL_PATTERNS = [
    # Clean up preprocessed patterns (remove if Google added extra words)
    (r'Sortie - Wanna do it\?.*', r'Wanna do Sortie?'),
    (r'Odyssey - Wanna do it\?.*', r'Wanna do Odyssey?'),
    (r'Omen - Wanna do it\?.*', r'Wanna do Omen?'),
    (r'Dyna - Wanna do it\?.*', r'Wanna do Dyna?'),
    (r'Ambu - Wanna do it\?.*', r'Wanna do Ambu?'),
    (r'(\w+) - Wanna go\?.*', r'Wanna go \1?'),
    (r'(\w+) - Let\'s go.*', r'Let\'s go \1'),
    
    # Formal questions â†’ Casual invites
    (r'Are you going to do (a |an )?(.+)\?', r'Wanna do \2?'),
    (r'Are you going to (.+)\?', r'Wanna \1?'),
    (r'Do you want to do (a |an )?(.+)\?', r'Wanna do \2?'),
    (r'Do you want to (.+)\?', r'Wanna \1?'),
    (r'Will you do (a |an )?(.+)\?', r'Wanna do \2?'),
    (r'Would you like to (.+)\?', r'Wanna \1?'),
    (r'Shall we do (a |an )?(.+)\?', r'Wanna do \2?'),
    (r'Shall we (.+)\?', r'Wanna \1?'),
    (r'Can you (.+)\?', r'Can you \1?'),
    (r'Could you (.+)\?', r'Can you \1?'),
    
    # Party/recruitment casual
    (r'Looking for group', r'LFG'),
    (r'Looking for more', r'LFM'),
    (r'Looking for party', r'LFP'),
    (r'Party member', r'PT member'),
    
    # Article fixes for FFXI activities
    (r'do a Sortie', r'do Sortie'),
    (r'do an Odyssey', r'do Odyssey'),
    (r'do a Dynamis', r'do Dyna'),
    (r'do a Dyna', r'do Dyna'),
    (r'do an Ambuscade', r'do Ambu'),
    (r'do an Ambu', r'do Ambu'),
    (r'do an Omen', r'do Omen'),
    (r'do a Limbus', r'do Limbus'),
    (r'do a Vagary', r'do Vagary'),
    (r'go to Sortie', r'go Sortie'),
    (r'go to Odyssey', r'go Odyssey'),
    (r'go to Omen', r'go Omen'),
    
    # Common stiff phrases â†’ casual
    (r'What about', r'How about'),
    (r'It is good', r'Sounds good'),
    (r'Is it good', r'Sound good'),
    (r'That is good', r'That\'s good'),
    (r'very much', r'lots'),
    (r'I understand', r'Got it'),
    (r'Understood', r'Got it'),
    (r'I will', r'I\'ll'),
    (r'I am', r'I\'m'),
    (r'We will', r'We\'ll'),
    (r'We are', r'We\'re'),
    
    # Politeness markers (remove excessive formality)
    (r'please be so kind', r'please'),
    (r'if you would be so kind', r'pls'),
    (r'thank you very much', r'thanks'),
    (r'I appreciate it', r'thanks'),
    
    # Gaming-specific casual
    (r'Experience points', r'XP'),
    (r'experience', r'exp'),
]

# ============================================================================
# COMMUNITY GLOSSARY SYSTEM
# ============================================================================

def load_community_glossary():
    """Load additional terms from community glossary file"""
    global glossary_last_modified
    
    if not COMMUNITY_GLOSSARY_FILE.exists():
        return {}
    
    try:
        current_modified = COMMUNITY_GLOSSARY_FILE.stat().st_mtime
        
        # Check if file was modified (hot-reload support)
        if current_modified == glossary_last_modified:
            return {}
        
        glossary_last_modified = current_modified
        
        community_terms = {}
        with open(COMMUNITY_GLOSSARY_FILE, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                
                # Skip comments and empty lines
                if not line or line.startswith('#'):
                    continue
                
                # Parse: Japanese|English
                if '|' in line:
                    parts = line.split('|', 1)
                    if len(parts) == 2:
                        jp_term, en_term = parts[0].strip(), parts[1].strip()
                        if jp_term and en_term:
                            community_terms[jp_term] = en_term
                        else:
                            print(f"[Kotoba] Warning: Empty term on line {line_num}")
                    else:
                        print(f"[Kotoba] Warning: Invalid format on line {line_num}: {line[:50]}")
        
        if community_terms:
            print(f"[Kotoba Translator] Loaded {len(community_terms)} community glossary terms")
        
        return community_terms
    except Exception as e:
        print(f"[Kotoba Translator] Error loading community glossary: {e}")
        return {}

def get_full_glossary():
    """Combine built-in and community glossaries"""
    full_glossary = FFXI_GLOSSARY.copy()
    community_terms = load_community_glossary()
    full_glossary.update(community_terms)  # Community terms override built-in
    return full_glossary

def has_japanese_chars(text):
    """Check if text contains Japanese characters"""
    if not text:
        return False
    
    for char in text:
        code = ord(char)
        # Hiragana, Katakana, Kanji ranges
        if (0x3040 <= code <= 0x309F or  # Hiragana
            0x30A0 <= code <= 0x30FF or  # Katakana
            0x4E00 <= code <= 0x9FFF or  # Kanji
            0x3400 <= code <= 0x4DBF):   # Kanji Extension A
            return True
    return False

def detect_untranslated_terms(original_text, translated_text, preprocessed_text):
    """Detect terms that might need glossary entries"""
    # Skip if translation failed
    if not translated_text or translated_text == original_text:
        return
    
    # Check if Japanese characters remain in translation (bad translation)
    if has_japanese_chars(translated_text):
        log_suggested_term(original_text, translated_text, "Japanese chars in translation")
        return
    
    # Check if translation is suspiciously identical to preprocessed (glossary might help)
    if translated_text.strip().lower() == preprocessed_text.strip().lower():
        # This means Google didn't really translate it, just returned what we sent
        if has_japanese_chars(original_text):
            log_suggested_term(original_text, translated_text, "No translation occurred")

def log_suggested_term(original, translation, reason):
    """Log potentially missing glossary terms"""
    try:
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        with open(SUGGESTED_TERMS_FILE, 'a', encoding='utf-8') as f:
            f.write(f"[{timestamp}] {reason}\n")
            f.write(f"  Original:    {original}\n")
            f.write(f"  Translation: {translation}\n")
            f.write(f"  Suggest: Add to ffxi_glossary.txt: {original}|<your_translation>\n")
            f.write(f"\n")
    except Exception as e:
        print(f"[Kotoba] Error logging suggested term: {e}")

# ============================================================================
# TRANSLATION FUNCTIONS
# ============================================================================

def preprocess_japanese(text):
    """Replace JP gaming terms/phrases before translation"""
    processed = text
    terms_used = 0
    
    # Special handling for common verb patterns with activities
    # Pattern: Activity + ã‚„ã‚‹/ã™ã‚‹/è¡Œã (wanna do/go X?)
    activity_verbs = {
        'ã‚„ã‚‹ï¼Ÿ': ' - Wanna do it?',
        'ã‚„ã‚': ' - Let\'s do it',
        'ã‚„ã‚‹': ' - doing',
        'è¡Œãï¼Ÿ': ' - Wanna go?',
        'è¡Œã“': ' - Let\'s go',
        'è¡Œã‹ãªã„ï¼Ÿ': ' - Wanna go?',
        'ã„ãï¼Ÿ': ' - Wanna go?',
        'ã™ã‚‹ï¼Ÿ': ' - Wanna do it?',
    }
    
    # Check if this is an activity + verb pattern
    for jp_verb, en_verb in activity_verbs.items():
        if jp_verb in processed:
            # Replace just the verb part, keep the activity name
            processed = processed.replace(jp_verb, en_verb)
            terms_used += 1
            break
    
    # Get full glossary (built-in + community)
    full_glossary = get_full_glossary()
    
    # Replace FFXI terms (sorted by length, longest first to avoid partial replacements)
    for jp_term in sorted(full_glossary.keys(), key=len, reverse=True):
        en_term = full_glossary[jp_term]
        if jp_term in processed:
            processed = processed.replace(jp_term, en_term)
            terms_used += 1
    
    # Track glossary usage
    if terms_used > 0:
        stats['glossary_terms_used'] += terms_used
    
    return processed

def postprocess_english(text):
    """Make translation more casual/natural"""
    processed = text
    for pattern, replacement in CASUAL_PATTERNS:
        processed = re.sub(pattern, replacement, processed, flags=re.IGNORECASE)
    return processed

def translate_text(text, source_lang, target_lang):
    """Translate text using DeepL with caching and gaming context"""
    cache_key = f"{source_lang}_{target_lang}_{text}"
    
    # Check cache
    if cache_key in cache:
        stats['cache_hits'] += 1
        print(f"[Kotoba Translator] âš¡ Cache hit!")
        return cache[cache_key]
    
    stats['cache_misses'] += 1
    stats['translations'] += 1
    
    try:
        # Preprocess: Replace FFXI/MMO terms BEFORE sending to DeepL
        preprocessed = text
        if source_lang == 'ja':
            preprocessed = preprocess_japanese(text)
            if preprocessed != text:
                print(f"[Kotoba Translator] ğŸ“ Preprocessed: {text[:40]} â†’ {preprocessed[:40]}")
        
        # Convert language codes for DeepL
        # DeepL uses 'JA' for Japanese, 'EN-US' for English
        deepl_source = 'JA' if source_lang == 'ja' else source_lang.upper()
        deepl_target = 'EN-US' if target_lang == 'en' else target_lang.upper()
        
        # Translate with DeepL
        result = translator.translate_text(preprocessed, source_lang=deepl_source, target_lang=deepl_target)
        translation = result.text
        
        # Postprocess: Make it casual/natural
        if target_lang == 'en':
            translation = postprocess_english(translation)
        
        # Detect potentially missing glossary terms
        if source_lang == 'ja' and target_lang == 'en':
            detect_untranslated_terms(text, translation, preprocessed)
        
        # Cache result
        cache[cache_key] = translation
        
        print(f"[Kotoba Translator] âœ“ {text[:40]} â†’ {translation[:40]}")
        return translation
    except Exception as e:
        print(f"[Kotoba Translator] âœ— Translation error: {e}")
        return None

def process_queue():
    """Process translation queue"""
    if not QUEUE_FILE.exists():
        return
    
    try:
        # Read all lines with multiple encoding fallbacks
        lines = []
        try:
            with open(QUEUE_FILE, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except UnicodeDecodeError:
            # Fallback to shift-jis (FFXI uses this)
            try:
                with open(QUEUE_FILE, 'r', encoding='shift-jis') as f:
                    lines = f.readlines()
            except UnicodeDecodeError:
                # Last resort: read as binary and decode with errors='replace'
                with open(QUEUE_FILE, 'rb') as f:
                    content = f.read()
                    lines = content.decode('utf-8', errors='ignore').split('\n')
        
        if not lines:
            return
        
        # Clear queue file immediately
        with open(QUEUE_FILE, 'w', encoding='utf-8') as f:
            f.write('')
        
        # Process each translation request
        results = []
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            try:
                # Parse: ID|SOURCE_LANG|TARGET_LANG|TEXT
                parts = line.split('|', 3)
                if len(parts) != 4:
                    print(f"[Kotoba Translator] Invalid format: {line[:50]}")
                    continue
                
                translation_id, source_lang, target_lang, text = parts
                
                # Unescape special characters
                text = text.replace('\\|', '|').replace('\\n', '\n').replace('\\r', '\r')
                
                # Clean up any null bytes or invalid characters
                text = text.replace('\x00', '').strip()
                
                if not text:
                    continue
                
                print(f"[Kotoba Translator] Translating ({source_lang}â†’{target_lang}): {text[:80]}")
                
                # Translate
                translation = translate_text(text, source_lang, target_lang)
                
                if translation:
                    # Escape pipes in translation result
                    translation = translation.replace('|', '\\|')
                    results.append(f"{translation_id}|{translation}\n")
            except Exception as e:
                print(f"[Kotoba Translator] Error processing line: {e}")
                import traceback
                traceback.print_exc()
        
        # Write results
        if results:
            with open(RESULTS_FILE, 'a', encoding='utf-8') as f:
                f.writelines(results)
            print(f"[Kotoba Translator] Wrote {len(results)} translation(s)")
    
    except Exception as e:
        print(f"[Kotoba Translator] Error processing queue: {e}")

def print_stats():
    """Print translation statistics"""
    uptime = int(time.time() - stats['start_time'])
    hours, remainder = divmod(uptime, 3600)
    minutes, seconds = divmod(remainder, 60)
    
    cache_total = stats['cache_hits'] + stats['cache_misses']
    cache_rate = (stats['cache_hits'] / cache_total * 100) if cache_total > 0 else 0
    
    print("\n" + "="*60)
    print("ğŸ“Š KOTOBA TRANSLATOR STATS")
    print("="*60)
    print(f"â±ï¸  Uptime:            {hours}h {minutes}m {seconds}s")
    print(f"ğŸ“ Translations:       {stats['translations']}")
    print(f"âš¡ Cache hits:         {stats['cache_hits']} ({cache_rate:.1f}%)")
    print(f"ğŸ” Cache misses:       {stats['cache_misses']}")
    print(f"ğŸ“š Glossary terms used: {stats['glossary_terms_used']}")
    print(f"ğŸ’¾ Cache size:         {len(cache)} entries")
    
    # Community glossary info
    community_terms = load_community_glossary()
    if community_terms:
        print(f"ğŸŒ Community glossary: {len(community_terms)} custom terms loaded")
    
    print("="*60 + "\n")

def create_example_glossary():
    """Create example community glossary file if it doesn't exist"""
    if not COMMUNITY_GLOSSARY_FILE.exists():
        try:
            with open(COMMUNITY_GLOSSARY_FILE, 'w', encoding='utf-8') as f:
                f.write("""# Kotoba Community Glossary
# Format: Japanese|English
# Lines starting with # are comments
#
# Add your own FFXI terms here!
# These will override built-in glossary terms.
# Hot-reloads automatically - no need to restart translator!
#
# Examples:
# ã‚¨ãƒ¼ãƒ™ãƒ«|Aeonic
# ã‚¢ãƒ¬ã‚­|Alexandrite
# ãƒ¡ãƒªãƒ|merit party
# å€‰åº«|mule
#
# Add your terms below:

""")
            print(f"[Kotoba Translator] Created example glossary: {COMMUNITY_GLOSSARY_FILE}")
        except Exception as e:
            print(f"[Kotoba Translator] Could not create example glossary: {e}")

def main():
    """Main loop"""
    print("\n" + "="*60)
    print("ğŸŒ¸ KOTOBA TRANSLATOR - ADVANCED EDITION ğŸŒ¸")
    print("="*60)
    print(f"ğŸ“‚ Queue:         {QUEUE_FILE}")
    print(f"ğŸ“‚ Results:       {RESULTS_FILE}")
    print(f"ğŸ“š Community:     {COMMUNITY_GLOSSARY_FILE}")
    print(f"ğŸ’¡ Suggestions:   {SUGGESTED_TERMS_FILE}")
    print("="*60)
    print("âœ¨ Features:")
    print("  - 100+ built-in FFXI terms")
    print("  - Hot-reload community glossary")
    print("  - Untranslated term detection")
    print("  - Translation caching")
    print("  - Casual tone post-processing")
    print("="*60)
    print("\nPress Ctrl+C to stop and see stats\n")
    
    # Ensure files exist
    QUEUE_FILE.touch(exist_ok=True)
    RESULTS_FILE.touch(exist_ok=True)
    create_example_glossary()
    
    # Load community glossary on startup
    community_terms = load_community_glossary()
    if community_terms:
        print(f"[Kotoba Translator] âœ“ Loaded {len(community_terms)} community terms\n")
    
    last_stats_print = time.time()
    
    try:
        while True:
            process_queue()
            
            # Print stats every 5 minutes
            if time.time() - last_stats_print > 300:
                print_stats()
                last_stats_print = time.time()
            
            time.sleep(0.5)  # Check twice per second
    except KeyboardInterrupt:
        print("\n[Kotoba Translator] Stopping...")
        print_stats()
        sys.exit(0)

if __name__ == "__main__":
    main()

